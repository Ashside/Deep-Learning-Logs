{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T13:10:53.150722Z",
     "start_time": "2025-08-04T13:10:48.247418Z"
    }
   },
   "source": [
    "#matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from setuptools.command.saveopts import saveopts\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:41:39.590576Z",
     "start_time": "2025-08-05T09:41:39.584017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"\n",
    "    Generate y = Xw + b + noise \\n\n",
    "    生成线性回归的合成数据集 \\n\n",
    "    # w: 权重向量，形状为 (n,1) \\n\n",
    "    # b: 偏置项，标量 \\n\n",
    "    # num_examples: 样本数量\n",
    "    \"\"\"\n",
    "\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "\n",
    "\n",
    "    # 返回特征和标签\n",
    "    # X: 特征矩阵，形状为 (num_examples, len(w))\n",
    "    # y: 标签向量，形状为 (num_examples,)\n",
    "    # 将y转换为列向量\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "def data_iter(batch_size, _features, _labels):\n",
    "    \"\"\"\n",
    "    生成数据迭代器，每次从数据样本中抽取少量样本用于更新模型参数 \\n\n",
    "    使用yield关键字返回一个生成器对象 \\n\n",
    "    yield 关键字的作用是将函数变成生成器（generator），每次调用生成器的 __next__() 方法（如在 for 循环中），就会从上一次 yield 处继续执行，返回一个批次的数据，而不是一次性返回所有数据。这样可以节省内存，并且方便处理大规模数据集。\\n\n",
    "    # batch_size: 批量大小\n",
    "    # features: 特征矩阵\n",
    "    # labels: 标签向量\n",
    "    \"\"\"\n",
    "    num_examples = len(_features)\n",
    "    index = list(range(num_examples))\n",
    "    random.shuffle(index)\n",
    "\n",
    "    # 将特征和标签按照随机顺序重新排列\n",
    "    # 根据batch_size将数据分成小批量\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 计算当前批次的索引\n",
    "        # min(i + batch_size, num_examples) 确保不会超出范围\n",
    "        batch = torch.tensor(index[i:min(i + batch_size, num_examples)])\n",
    "        # index_select(0, batch_index) 从第0维（行）选择指定的索引\n",
    "        # 返回一个新的张量，其中包含指定索引的行\n",
    "        # yield 返回一个生成器对象，每次迭代返回一个批次的特征和标签\n",
    "        # 下次调用时会从上次的yield处继续执行\n",
    "        yield _features.index_select(0, batch), _labels.index_select(0, batch)\n",
    "\n"
   ],
   "id": "1bd884059390eb81",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:11:00.470364Z",
     "start_time": "2025-08-04T13:11:00.464421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n"
   ],
   "id": "8c4eac94e58e67df",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:11:01.284621Z",
     "start_time": "2025-08-04T13:11:01.263085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X,y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break  # 只打印第一个批次的数据"
   ],
   "id": "ddd464d9adab22c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1194,  1.3199],\n",
      "        [-0.2612, -1.1244],\n",
      "        [-0.5767,  0.3896],\n",
      "        [-0.4884, -1.0144],\n",
      "        [-1.8603,  0.5096],\n",
      "        [ 0.0821,  1.3456],\n",
      "        [-0.3518,  0.7616],\n",
      "        [-0.1881,  0.3565],\n",
      "        [-0.6909, -0.7781],\n",
      "        [ 1.6174,  0.9306]]) \n",
      " tensor([[-2.5369],\n",
      "        [ 7.4994],\n",
      "        [ 1.7232],\n",
      "        [ 6.6892],\n",
      "        [-1.2628],\n",
      "        [-0.2121],\n",
      "        [ 0.9193],\n",
      "        [ 2.6291],\n",
      "        [ 5.4752],\n",
      "        [ 4.2654]])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义模型、损失函数、优化器",
   "id": "202d812c172647ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T14:26:07.505545Z",
     "start_time": "2025-08-04T14:26:07.499536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_reg(X, w, b):\n",
    "    \"\"\"\n",
    "    线性回归模型 \\n\n",
    "    # X: 特征矩阵，形状为 (num_examples, n) \\n\n",
    "    # w: 权重向量，形状为 (n,1) \\n\n",
    "    # b: 偏置项，标量 \\n\n",
    "    \"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    均方误差损失函数 \\n\n",
    "    # y_hat: 模型预测值，形状为 (num_examples, 1) \\n\n",
    "    # y: 真实标签，形状为 (num_examples, 1) \\n\n",
    "    \"\"\"\n",
    "    return (y_hat - y.view(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "def sgd(params, learn_rate, batch_size):\n",
    "    \"\"\"\n",
    "    随机梯度下降优化器 \\n\n",
    "    公式: \\n\n",
    "    w = w - learn_rate * (1/batch_size) * ∇L(w) \\n\n",
    "\n",
    "    # params: 模型参数列表 \\n\n",
    "    # learn_rate: 学习率 \\n\n",
    "    # batch_size: 批量大小 \\n\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "        # 遍历每个参数，更新其值\n",
    "        for param in params:\n",
    "            # 使用梯度下降更新参数\n",
    "            # param.grad: 当前参数的梯度\n",
    "            # learn_rate: 学习率\n",
    "            # batch_size: 批量大小\n",
    "            param -= learn_rate * param.grad / batch_size\n",
    "            param.grad.zero_()  # 清除梯度"
   ],
   "id": "20a086a06aa175a7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "训练过程中，将进行如下迭代\n",
    "- 初始化参数\n",
    "- 重复以下步骤\n",
    "    - 计算梯度\n",
    "    - 更新参数"
   ],
   "id": "8d8cbdbd19ffdee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T14:46:41.374006Z",
     "start_time": "2025-08-04T14:46:41.113009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "lr = 0.03\n",
    "num_epochs = 10\n",
    "net = linear_reg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # 前向传播\n",
    "        y_hat = net(X, w, b)\n",
    "        # 计算损失\n",
    "        l = loss(y_hat, y)\n",
    "        # 反向传播\n",
    "        l.sum().backward()\n",
    "        # 更新参数\n",
    "        sgd([w, b], lr, batch_size)\n",
    "\n",
    "    # 打印每个epoch的损失\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {train_l.mean():f}')\n",
    "\n",
    "print(f'w的估计误差为：{true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差为：{true_b - b}')"
   ],
   "id": "1a336e96d0ecf025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.037658\n",
      "epoch 2, loss 0.000140\n",
      "epoch 3, loss 0.000052\n",
      "epoch 4, loss 0.000052\n",
      "epoch 5, loss 0.000052\n",
      "epoch 6, loss 0.000052\n",
      "epoch 7, loss 0.000052\n",
      "epoch 8, loss 0.000052\n",
      "epoch 9, loss 0.000052\n",
      "epoch 10, loss 0.000052\n",
      "w的估计误差为：tensor([0.0004, 0.0005], grad_fn=<SubBackward0>)\n",
      "b的估计误差为：tensor([-0.0002], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "重启内核",
   "id": "783e3ead0d3a7dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "线性回归的简洁实现",
   "id": "af000abd8b9e33f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:47:23.675050Z",
     "start_time": "2025-08-05T09:47:23.664484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n"
   ],
   "id": "45e13975132293a2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:47:24.036129Z",
     "start_time": "2025-08-05T09:47:24.017090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ],
   "id": "c112860d64f623c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:47:24.385235Z",
     "start_time": "2025-08-05T09:47:24.375715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    将数据加载到DataLoader中 \\n\n",
    "    通过DataLoader可以方便地迭代数据 \\n\n",
    "    在每个迭代中，DataLoader会返回一个批次的数据 \\n\n",
    "    # data_arrays: 数据数组，包含特征和标签 \\n\n",
    "    # batch_size: 批量大小 \\n\n",
    "    # is_train: 是否为训练数据 \\n\n",
    "    \"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)  # 创建TensorDataset\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)  # 返回DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "# 使用load_array函数加载数据\n",
    "# 获得一个DataLoader对象，可以用于迭代数据\n",
    "data_iter = load_array((features, labels), batch_size)"
   ],
   "id": "7f04653dee314ed4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:47:27.878945Z",
     "start_time": "2025-08-05T09:47:25.133743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import  nn\n",
    "# 定义线性回归模型\n",
    "# 使用Sequential容器来构建模型 \\n\n",
    "# nn.Linear(2, 1) 表示输入特征维度为2，输出特征维度为1 \\n\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,1))\n",
    "\n",
    "net[0].weight.data.normal_(0, 0.01)  # 初始化权重\n",
    "net[0].bias.data.fill_(0)  # 初始化偏置\n",
    "\n",
    "loss = nn.MSELoss()  # 定义损失函数为均方误差损失\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)  # 定义优化器为随机梯度下降\n"
   ],
   "id": "9aa13d6693e3352",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "训练模型",
   "id": "5381b39141905b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:47:52.432438Z",
     "start_time": "2025-08-05T09:47:52.224964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # 前向传播\n",
    "        y_hat = net(X)\n",
    "        # 计算损失\n",
    "        l = loss(y_hat, y)\n",
    "        # 反向传播\n",
    "        trainer.zero_grad()  # 清除梯度\n",
    "        l.backward()  # 计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "\n",
    "    # 打印每个epoch的损失\n",
    "    train_l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {train_l.mean():f}')"
   ],
   "id": "46df5eeef2d04f66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000327\n",
      "epoch 2, loss 0.000097\n",
      "epoch 3, loss 0.000097\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:53:51.165660Z",
     "start_time": "2025-08-05T09:53:51.151835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = net[0].weight.data\n",
    "b = net[0].bias.data\n",
    "print(f'w的估计误差为：{true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差为：{true_b - b}')"
   ],
   "id": "a179543bdd90659e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差为：tensor([-0.0005,  0.0001])\n",
      "b的估计误差为：tensor([0.0001])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:57:10.198764Z",
     "start_time": "2025-08-05T09:57:09.685587Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "332e3e18c8c26b21",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnet\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mF:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:133\u001B[0m, in \u001B[0;36mSequential.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(OrderedDict(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems())[idx]))\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_item_by_idx\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:124\u001B[0m, in \u001B[0;36mSequential._get_item_by_idx\u001B[1;34m(self, iterator, idx)\u001B[0m\n\u001B[0;32m    122\u001B[0m idx \u001B[38;5;241m=\u001B[39m operator\u001B[38;5;241m.\u001B[39mindex(idx)\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m-\u001B[39msize \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m idx \u001B[38;5;241m<\u001B[39m size:\n\u001B[1;32m--> 124\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is out of range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    125\u001B[0m idx \u001B[38;5;241m%\u001B[39m\u001B[38;5;241m=\u001B[39m size\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(islice(iterator, idx, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "\u001B[1;31mIndexError\u001B[0m: index 1 is out of range"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
